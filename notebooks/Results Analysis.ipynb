{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab00e968",
   "metadata": {},
   "source": [
    "# Results Analysis\n",
    "\n",
    "This notebook serves analysing LADDS run results, the following packages need to be installed\n",
    "\n",
    "`conda install tqdm pykep pandas scikit-learn h5py -c conda-forge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Append main folder\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import math\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pykep as pk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import h5py\n",
    "import yaml\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "#%matplotlib notebook\n",
    "\n",
    "dt = 10 #timestep of the inspected simulations, affects time labels in plots\n",
    "starting_t = pk.epoch_from_string('2022-01-01 00:00:00.000') # starting t of the simulation\n",
    "filter_CC = True # filter Constellation Collisions\n",
    "nConstellations = 9\n",
    "cnames = [\"Debris\",\"Astra1\",\"Astra2\",\"Astra3\",\"OneWeb1\",\"OneWeb2\",\"Starlink1\",\"Kuiper\",\"Telesat\",\"GuoWang\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca306edd",
   "metadata": {},
   "source": [
    "### Load the run data\n",
    "Adapt the path below to the h5 file you want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a1d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending h5 files\n",
    "#with h5py.File('../../14ySim/simulationData1C.h5', 'a') as hf:\n",
    "#    file2 = h5py.File(\"../../14ySim/simulationData2A.h5\",'r')\n",
    "#    for pData in file2[\"ParticleData\"].values():\n",
    "#        hf.copy(pData,hf[\"ParticleData\"])\n",
    "#        \n",
    "#    for cData in file2[\"CollisionData\"].values():\n",
    "#        hf.copy(cData,hf[\"CollisionData\"])\n",
    "    \n",
    "#print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hdf5 file\n",
    "data = h5py.File(\"../../14ySim/simulationData1C.h5\")\n",
    "\n",
    "# Determine the iterations at which output was written\n",
    "iterations_idx = list(data[\"ParticleData\"].keys())\n",
    "iterations_idx = [int(it) for it in iterations_idx]\n",
    "iterations_idx.sort()\n",
    "max_iterations = max(iterations_idx)\n",
    "print(\"Found a total of\", max_iterations, \" iterations.\")\n",
    "\n",
    "# Find out the simulation runtime in days\n",
    "end_t = pk.epoch(starting_t.mjd2000 + max_iterations * dt * pk.SEC2DAY)\n",
    "total_days = end_t.mjd - starting_t.mjd\n",
    "print(\"Simulation ran for a total of\", total_days, \" days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c22a24",
   "metadata": {},
   "source": [
    "### Extract the positions, velocities and corresponding particle IDs from the hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e8f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs,vs,ids = [],[],[] #will hold r,v, ids for whole simulation\n",
    "\n",
    "for idx in tqdm(iterations_idx):\n",
    "    \n",
    "    # Load velocities\n",
    "    v_x = data[\"ParticleData\"][str(idx)][\"Particles\"][\"Velocities\"][:][\"x\"]\n",
    "    v_y = data[\"ParticleData\"][str(idx)][\"Particles\"][\"Velocities\"][:][\"y\"]\n",
    "    v_z = data[\"ParticleData\"][str(idx)][\"Particles\"][\"Velocities\"][:][\"z\"]\n",
    "    v = np.vstack([v_x,v_y,v_z]).transpose()\n",
    "    \n",
    "    # Load positions\n",
    "    r_x = data[\"ParticleData\"][str(idx)][\"Particles\"][\"Positions\"][:][\"x\"]\n",
    "    r_y = data[\"ParticleData\"][str(idx)][\"Particles\"][\"Positions\"][:][\"y\"]\n",
    "    r_z = data[\"ParticleData\"][str(idx)][\"Particles\"][\"Positions\"][:][\"z\"]\n",
    "    r = np.vstack([r_x,r_y,r_z]).transpose()\n",
    "    \n",
    "    # Get IDs\n",
    "    ID = np.array(data[\"ParticleData\"][str(idx)][\"Particles\"][\"IDs\"])\n",
    "    \n",
    "    # Store in single large list\n",
    "    rs.append(r * 1000.) #convert to m \n",
    "    vs.append(v * 1000.) #convert to m \n",
    "    ids.append(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6869613",
   "metadata": {},
   "outputs": [],
   "source": [
    "idCounts = [0] * (nConstellations+1)\n",
    "ids_int = [int(id) for id in ids[len(iterations_idx)-1]]\n",
    "for id in ids_int:\n",
    "    idCounts[id // 1000000] = idCounts[id // 1000000] + 1\n",
    "for i in range(len(idCounts)):\n",
    "    print(str(i) + \": \" + str(idCounts[i]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa988ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding constellation satellites\n",
    "itera = 54\n",
    "idxx = []\n",
    "idss = []\n",
    "added = 0\n",
    "for i in range(len(ids[itera])):\n",
    "    if(ids[itera][i] >= 6000000 and ids[itera][i] <= 7000000):\n",
    "        idxx.append(i)\n",
    "        idss.append(ids[itera][i])\n",
    "        added += 1\n",
    "        if(ids[itera][i] == 6002944):\n",
    "            print(\"existiert\")\n",
    "        \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing altitude of satellite with certain id\n",
    "pos = [0] * 3\n",
    "for i in range(len(idss)):\n",
    "    idd = idss[i]\n",
    "    if(idd == 6002944):\n",
    "        pos[0] = rs[itera][idxx[i]][0] / 1000\n",
    "        pos[1] = rs[itera][idxx[i]][1] / 1000\n",
    "        pos[2] = rs[itera][idxx[i]][2] / 1000\n",
    "        print(idxx[i])\n",
    "        print(ids[itera][idxx[i]])\n",
    "        print(np.linalg.norm(pos)-6371)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54d50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print ids and corresponding altitudes at iteration 0\n",
    "added = 0\n",
    "pos = [0] * 3\n",
    "for i in idxx:\n",
    "    pos[0] = rs[0][i][0] / 1000\n",
    "    pos[1] = rs[0][i][1] / 1000\n",
    "    pos[2] = rs[0][i][2] / 1000\n",
    "    print(i)\n",
    "    print(ids[0][i])\n",
    "    print(np.linalg.norm(pos)-6371)\n",
    "    added += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545b66b",
   "metadata": {},
   "source": [
    "### Now some helper functions for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31024da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many iterations happened between IO\n",
    "iteration_stepsize = iterations_idx[1] - iterations_idx[0]\n",
    "\n",
    "\n",
    "def find_closest_it(array,value):\n",
    "    \"\"\" Returns the index of the closest iteration to the passed iteration value. \n",
    "    E.g if saved iteration were [0,1000,2000] and you pass 500 , it will return 0, for 501 returns 1 etc.\"\"\"\n",
    "    idx = np.searchsorted(array, value+ (iteration_stepsize // 2), side=\"left\")\n",
    "    if idx > 0 and (idx == len(array) or math.fabs(value - array[idx-1]) < math.fabs(value - array[idx])):\n",
    "        return idx - 1\n",
    "    else:\n",
    "        return idx - 1\n",
    "    \n",
    "def get_particle_r_v(ID,it):\n",
    "    \"\"\"From the large rs, vs arrays returns the values particle with a certain ID closest to the passed iteration\"\"\"\n",
    "    it = find_closest_it(iterations_idx,it)\n",
    "    idx = np.argmax(ids[it]==ID)\n",
    "    return rs[it][idx],vs[it][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da666df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for conjunction tracking as used in simulation\n",
    "# Determines what is in the plots\n",
    "thresholds = [1,5,10,20,25,50,100,250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56283033",
   "metadata": {},
   "source": [
    "### Extract the conjunctions from the HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32649b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert conjunctions to a pandas dataframe for convenience\n",
    "conj = pd.DataFrame(columns=[\"P1\",\"P2\",\"Iteration\",\"SquaredDistance\"])\n",
    "cconjs = []\n",
    "for i in range(nConstellations+1):\n",
    "    cconj = pd.DataFrame(columns=[\"P1\",\"P2\",\"Iteration\",\"SquaredDistance\"])\n",
    "    cconjs.append(cconj)\n",
    "collision_keys = data[\"CollisionData\"].keys()\n",
    "for it in tqdm(collision_keys):\n",
    "    iteration = int(it)\n",
    "    collisions = data[\"CollisionData\"][it][\"Collisions\"]\n",
    "    for collision in collisions:\n",
    "        if(filter_CC and collision[0] // 1000000 != 0 and collision[1] // 1000000 != 0):\n",
    "            continue\n",
    "        centry = {\"P1\":collision[0],\n",
    "                  \"P2\": collision[1],\n",
    "                  \"Iteration\": iteration,\n",
    "                  \"SquaredDistance\": collision[2]}\n",
    "        conj = conj.append(centry,ignore_index=True)\n",
    "        # also create buckets for each constellation\n",
    "        cconjs[collision[0] // 1000000] = cconjs[collision[0] // 1000000].append(centry,ignore_index=True)\n",
    "        cconjs[collision[1] // 1000000] = cconjs[collision[1] // 1000000].append(centry,ignore_index=True)\n",
    "conj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccca74",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "Throw away all but the closest encounter, also compute actual distance (not squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort ascending by conjunction distance, then drop all but the first to get closest encounter\n",
    "conj = conj.sort_values('SquaredDistance', ascending=True)\n",
    "unique_conj = conj.drop_duplicates(subset=[\"P1\",\"P2\"],keep=\"first\")\n",
    "unique_conj[\"Distance\"] = np.sqrt(unique_conj.SquaredDistance) * 1000. # Compute distance in meters\n",
    "del conj # deleting to not accidentally use it\n",
    "\n",
    "unique_cconjs = []\n",
    "for i in range(nConstellations+1):\n",
    "    unique_cconj = pd.DataFrame(columns=[\"P1\",\"P2\",\"Iteration\",\"SquaredDistance\"])\n",
    "    unique_cconjs.append(unique_cconj)\n",
    "    \n",
    "#also clean up buckets\n",
    "for i in range(len(cconjs)):\n",
    "    cconjs[i] = cconjs[i].sort_values('SquaredDistance', ascending=True)\n",
    "    unique_cconjs[i] = cconjs[i].drop_duplicates(subset=[\"P1\",\"P2\"],keep=\"first\")\n",
    "    unique_cconjs[i][\"Distance\"] = np.sqrt(unique_cconjs[i].SquaredDistance) * 1000. # Compute distance in meters\n",
    "\n",
    "all_conj = unique_conj\n",
    "unique_conj\n",
    "print(len(unique_conj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae0da2",
   "metadata": {},
   "source": [
    "## Let the plotting begin.\n",
    "We start with counting the number of conjunctions for different thresholds over the simulation time and then threshold vs total # of conjunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e35ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "idss = [int(id) for id in ids[0]]\n",
    "rss = [r for r in rs[0]]\n",
    "\n",
    "min_altitude = 200\n",
    "max_altitude = 2000\n",
    "num_buckets = 100\n",
    "\n",
    "bucket_size = (max_altitude - min_altitude) / num_buckets\n",
    "altitude_axis = np.linspace(min_altitude,max_altitude,num_buckets)\n",
    "buckets = [0] * num_buckets\n",
    "for i in range(len(idss)):\n",
    "    if(idss[i]<1000000):\n",
    "        r = [rss[i][0]/1000,rss[i][1]/1000,rss[i][2]/1000]\n",
    "        altitude = np.sqrt(r[0]*r[0]+r[1]*r[1]+r[2]*r[2]) - 6371\n",
    "        if(altitude > min_altitude and altitude < max_altitude):\n",
    "            buckets[int((altitude-min_altitude) // bucket_size)] += 1 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,5),dpi=100)\n",
    "\n",
    "ax.bar(altitude_axis,buckets,bucket_size+0.5,label=\"altitude\")\n",
    "\n",
    "ax.set_xlabel(\"Altitude (km)\")\n",
    "ax.set_ylabel(\"# of objects\")\n",
    "ax.set_title(\"distribution of objects by altitude\")\n",
    "plt.show()\n",
    "\n",
    "print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the plot base: all conjunctions (all_conj), debris-debris conjunctions (unique_cconj[0])\n",
    "# constellation n (unique_cconjs[n])\n",
    "unique_conj = all_conj\n",
    "\n",
    "# determine time granularity\n",
    "steps = 1000\n",
    "t = np.linspace(0,end_t.mjd - starting_t.mjd,steps)\n",
    "it = np.linspace(0,max_iterations,steps)\n",
    "summed_conjs = [[] for _ in thresholds]\n",
    "\n",
    "# Compute number of conjunctions\n",
    "for i in tqdm(it):\n",
    "    for idx,threshold in enumerate(thresholds):\n",
    "        count = len(unique_conj[(unique_conj.Iteration < i) & (unique_conj.Distance < threshold)])\n",
    "        summed_conjs[idx].append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9628720",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,5),dpi=100)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# Iterate over thresholds and plot for each\n",
    "for idx,row in enumerate(thresholds):\n",
    "    plt.plot(t,summed_conjs[idx],linewidth=3)\n",
    "    \n",
    "plt.legend([str(t) + \"m\" for t in thresholds],loc='upper center', bbox_to_anchor=(1.1,0.8), ncol=1, fancybox=True, shadow=True)\n",
    "plt.title(\"Conjunction Thresholds Comparison\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"# of Conjunctions\")\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b5575",
   "metadata": {},
   "source": [
    "## Stack Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c112e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_threshold = 100\n",
    "\n",
    "steps = 100\n",
    "t = np.linspace(0,end_t.mjd - starting_t.mjd,steps)\n",
    "it = np.linspace(0,max_iterations,steps)\n",
    "summed_conjs = [[] for _ in range(nConstellations+1)]\n",
    "stepsizeSB = (end_t.mjd - starting_t.mjd)/steps + 0.5\n",
    "for i in tqdm(it):\n",
    "    for c_idx in range(nConstellations+1):\n",
    "        count = len(unique_cconjs[c_idx][(unique_cconjs[c_idx].Iteration < i)&\n",
    "                                         (unique_cconjs[c_idx].Distance < sb_threshold)])\n",
    "        summed_conjs[c_idx].append(count)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,5),dpi=100)\n",
    "\n",
    "acc = np.array(summed_conjs[1])\n",
    "ax.bar(t,summed_conjs[1],stepsizeSB,label=cnames[1])\n",
    "for c_idx in range(2,10):\n",
    "    ax.bar(t,summed_conjs[c_idx],stepsizeSB,label=cnames[c_idx],bottom=acc)\n",
    "    acc = acc + np.array(summed_conjs[c_idx])\n",
    "\n",
    "ax.set_xlabel(\"Days\")\n",
    "ax.set_ylabel(\"# of Conjunctions (\" + str(sb_threshold) + \"m)\")\n",
    "ax.set_title(\"Total Conjunctions by Constellation\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "print(len(unique_cconjs[2][(unique_cconjs[2].Iteration < it[12])]))\n",
    "print(summed_conjs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute conjunction counts at specific points in the simulation\n",
    "steps = 1000 # threshold granularity\n",
    "max_threshold = 10 # maximum investigated threshold\n",
    "# Time axis\n",
    "threshold_grid = np.logspace(-1,np.log10(max_threshold),steps)\n",
    "sums = []\n",
    "for idx,threshold in enumerate(threshold_grid):\n",
    "    count = len(unique_conj[(unique_conj.Distance < threshold)])\n",
    "    sums.append(count)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5),dpi=100)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "plt.plot(threshold_grid,sums,linewidth=3)\n",
    "    \n",
    "plt.title(\"Conjunction Thresholds vs. \\n # of Conjunctions\")\n",
    "plt.xlabel(\"Threshold [m]\")\n",
    "plt.ylabel(\"# of Conjunctions\")\n",
    "# plt.gca().set_xscale(\"log\")\n",
    "# plt.gca().set_yscale(\"log\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f9afbd",
   "metadata": {},
   "source": [
    "### Let's investigate the relationship of orbital elements and conjunctions (below some threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd393a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = [[],[],[],[],[],[]]\n",
    "threshold = 200 #cutoff for investigated conjunctions\n",
    "abc = 0\n",
    "for idx,row in tqdm(unique_conj.iterrows(),total=len(unique_conj.index)):\n",
    "    if row.Distance < threshold:\n",
    "        abc += 1\n",
    "        r,v = get_particle_r_v(row.P1,row.Iteration)\n",
    "        if abc == 3:\n",
    "            print(row.P1)\n",
    "            print(r)\n",
    "            print(v)\n",
    "            print(row.P2)\n",
    "            print(r)\n",
    "            print(v)\n",
    "        a,e,i,W,w,E = pk.ic2par(r.astype(\"double\"),v.astype(\"double\"), pk.MU_EARTH)\n",
    "        elements[0].append(abs(a))\n",
    "        elements[1].append(abs(e))\n",
    "        elements[2].append(abs(i))\n",
    "        elements[3].append(abs(W))\n",
    "        elements[4].append(abs(w))\n",
    "        elements[5].append(abs(E))\n",
    "        \n",
    "        r,v = get_particle_r_v(row.P2,row.Iteration)\n",
    "        a,e,i,W,w,E = pk.ic2par(r.astype(\"double\"),v.astype(\"double\"), pk.MU_EARTH)\n",
    "        elements[0].append(abs(a))\n",
    "        elements[1].append(abs(e))\n",
    "        elements[2].append(abs(i))\n",
    "        elements[3].append(abs(W))\n",
    "        elements[4].append(abs(w))\n",
    "        elements[5].append(abs(E))\n",
    "    \n",
    "print(\"Collected data for \",len(elements[0]),\" conjunctions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ed433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute orbital elements for the entire population at t=0 as a reference\n",
    "pop_elements = [[],[],[],[],[],[]]\n",
    "for r_i,v_i in tqdm(zip(rs[0],vs[0])):\n",
    "    a,e,i,W,w,E = pk.ic2par(r_i.astype(\"double\"),v_i.astype(\"double\"), pk.MU_EARTH)\n",
    "    pop_elements[0].append(abs(a))\n",
    "    pop_elements[1].append(abs(e))\n",
    "    pop_elements[2].append(abs(i))\n",
    "    pop_elements[3].append(abs(W))\n",
    "    pop_elements[4].append(abs(w))\n",
    "    pop_elements[5].append(abs(E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf5195",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot histogram of collisions for each orbital element\n",
    "for idx,element in enumerate([\"a [m]\",\"e\",\"i [rad]\",\"W\",\"w\",\"E\"]):\n",
    "    fig = plt.figure(figsize=(6,4),dpi=100)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    bins = np.linspace(min(pop_elements[idx]),max(pop_elements[idx]),32)\n",
    "    plt.hist(elements[idx],bins=bins,density = True,alpha=0.5)\n",
    "    plt.hist(pop_elements[idx],bins=bins,density = True,alpha=0.5)\n",
    "    plt.legend([f\"Particles in conjunctions (< {threshold}m)\",\"All particles at t=0\"])\n",
    "    plt.title(\"Histogram of \"+element)\n",
    "    plt.xlabel(element)\n",
    "    plt.ylabel(\"Relative Frequency\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7fac8",
   "metadata": {},
   "source": [
    "### Now let's orbital elements of all particles over the simulation as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce842a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute min, max and mean of orbital elements over simulation\n",
    "min_elements = [[],[],[],[],[],[]]\n",
    "mean_elements = [[],[],[],[],[],[]]\n",
    "max_elements = [[],[],[],[],[],[]]\n",
    "\n",
    "steps = list(range(len(rs)))\n",
    "\n",
    "for idx in tqdm(steps[::1]): #pick every n-th step\n",
    "    elements = [[],[],[],[],[],[]]\n",
    "    v_it,r_it = vs[idx],rs[idx]\n",
    "    for v,r in zip(v_it,r_it):\n",
    "        a,e,i,W,w,E = pk.ic2par(r.astype(\"double\"),v.astype(\"double\"), pk.MU_EARTH)\n",
    "        elements[0].append(abs(a))\n",
    "        elements[1].append(abs(e))\n",
    "        elements[2].append(abs(i))\n",
    "        elements[3].append(abs(W))\n",
    "        elements[4].append(abs(w))\n",
    "        elements[5].append(abs(E))\n",
    "    for i in range(6):\n",
    "        min_elements[i].append(np.min(elements[i]))\n",
    "        mean_elements[i].append(np.mean(elements[i]))\n",
    "        max_elements[i].append(np.max(elements[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b8a1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Time axis of the simulation\n",
    "t = np.linspace(0,end_t.mjd - starting_t.mjd,len(mean_elements[0]))\n",
    "\n",
    "# Plot for each orbital element\n",
    "for idx,element in enumerate([\"a\",\"e\",\"i\",\"W\",\"w\",\"E\"]):\n",
    "    fig = plt.figure(figsize=(6,4),dpi=100)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(t,mean_elements[idx],linewidth=1)\n",
    "#     plt.plot(t,min_elements[idx],linewidth=1)\n",
    "#     plt.plot(t,max_elements[idx],linewidth=1)\n",
    "#     plt.legend([\"mean\",\"min\",\"max\"],loc='upper center', bbox_to_anchor=(1.1,0.8), ncol=1, fancybox=True, shadow=True)\n",
    "    plt.title(\"Evolution of \"+element)\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Mean \" + element)\n",
    "    plt.tight_layout()\n",
    "#         plt.gca().set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d9557",
   "metadata": {},
   "source": [
    "### Finally, we will plot the distribution of the distance to the next particle over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a KNN to get distance to nearest neighbors over simulation\n",
    "\n",
    "all_distances = []\n",
    "\n",
    "r_subset = rs[::1] #pick every n-th step\n",
    "\n",
    "for r_it in tqdm(r_subset,total=len(r_subset)):\n",
    "    elements = [[],[],[],[],[],[]]\n",
    "    knn = NearestNeighbors(n_neighbors=2).fit(r_it)\n",
    "    distances,_ = knn.kneighbors(r_it)\n",
    "    distances = distances[:,1] / 1000 # convert to km\n",
    "    all_distances.append(distances)\n",
    "    \n",
    "all_distances = np.asarray(all_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db736f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample to look only at those below some threshold\n",
    "small_distances = []\n",
    "max_dist = 25\n",
    "for dist in all_distances:\n",
    "    small_distances.append(dist[dist < max_dist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distributions for each iteration\n",
    "bins = 64\n",
    "x = np.linspace(0, max_dist, bins)\n",
    "y = np.linspace(0, total_days, len(all_distances))\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = []\n",
    "for dist in tqdm(small_distances):\n",
    "    hist,bin_vals = np.histogram(dist, bins = x,density=False)\n",
    "    hist = np.cumsum(hist)\n",
    "    hist = np.concatenate([hist,[hist[-1]]]) # last value remains for bucket\n",
    "    Z.append(hist)\n",
    "x = np.concatenate([[0],x])\n",
    "Z = np.asarray(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4a070",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create beautiful plots\n",
    "fig = plt.figure(figsize = (8,8),dpi=100)\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, Z,  rstride=1, cstride=1, cmap=\"plasma\", edgecolor='none')\n",
    "ax.view_init(elev=20., azim=120)\n",
    "ax.set_xlabel('Closest Distance [km]')\n",
    "ax.set_ylabel('Days')\n",
    "ax.set_zlabel('Cumulative Frequency');\n",
    "ax.set_xlim([0,max_dist])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e723fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
