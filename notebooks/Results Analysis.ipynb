{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab00e968",
   "metadata": {},
   "source": [
    "# Results Analysis\n",
    "\n",
    "This notebook serves analysing LADDS run results, the following packages need to be installed\n",
    "\n",
    "`conda install tqdm pykep pandas scikit-learn h5py -c conda-forge`\n",
    "\n",
    "In general, it will:\n",
    "\n",
    "1. Load the HDF data generated by a simulation and extract position, velocities, conjunctions.\n",
    "2. Create a bunch of plots related to conjunctions\n",
    "3. Investigate orbital dynamics (elements, distances between particles) over the simulation\n",
    "\n",
    "## Input \n",
    "\n",
    "This notebook uses the input from a simulation run in HDF5 format. For more details refer to the [docs](https://github.com/esa/LADDS#hdf5)\n",
    "\n",
    "## Output\n",
    "\n",
    "N/A at the moment but the generated plots may be saved as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Append main folder\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import math\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pykep as pk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "sns.set(font_scale = 1.5)\n",
    "from mpl_toolkits import mplot3d\n",
    "#%matplotlib notebook\n",
    "\n",
    "dt = 10 #timestep of the inspected simulations, affects time labels in plots\n",
    "starting_t = pk.epoch_from_string('2022-01-01 00:00:00.000') # starting t of the simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca306edd",
   "metadata": {},
   "source": [
    "## 1. Load the run data\n",
    "Adapt the path below to the h5 file you want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hdf5 file\n",
    "data = h5py.File(\"../results/year_00-06/01x01x28/simulationData_year_00-06.h5\")\n",
    "\n",
    "# Determine the iterations at which output was written\n",
    "iterations_idx_str = list(data[\"ParticleData\"].keys())\n",
    "iterations_idx = []\n",
    "for it in iterations_idx_str:\n",
    "    if it != \"ConstantProperties\":\n",
    "        iterations_idx.append(int(it))\n",
    "iterations_idx.sort()\n",
    "max_iterations = max(iterations_idx)\n",
    "print(\"Found a total of\", max_iterations, \" iterations.\")\n",
    "\n",
    "# Find out the simulation runtime in days\n",
    "end_t = pk.epoch(starting_t.mjd2000 + max_iterations * dt * pk.SEC2DAY)\n",
    "total_days = end_t.mjd - starting_t.mjd\n",
    "print(\"Simulation ran for a total of\", total_days, \" days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c22a24",
   "metadata": {},
   "source": [
    "### Extract the positions, velocities and corresponding particle IDs from the hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e8f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs,vs,ids = [],[],[] #will hold r,v, ids for whole simulation\n",
    "stepsize = 25\n",
    "iterations_idx = iterations_idx[::stepsize]\n",
    "for idx in tqdm(iterations_idx):\n",
    "    \n",
    "    # Load velocities\n",
    "    v_x = data[\"ParticleData\"][str(idx)][\"Particles\"][\"Velocities\"][:][\"x\"]\n",
    "    v_y = data[\"ParticleData\"][str(idx)][\"Particles\"][\"Velocities\"][:][\"y\"]\n",
    "    v_z = data[\"ParticleData\"][str(idx)][\"Particles\"][\"Velocities\"][:][\"z\"]\n",
    "    v = np.vstack([v_x,v_y,v_z]).transpose()\n",
    "    \n",
    "    # Load positions\n",
    "    r_x = data[\"ParticleData\"][str(idx)][\"Particles\"][\"Positions\"][:][\"x\"]\n",
    "    r_y = data[\"ParticleData\"][str(idx)][\"Particles\"][\"Positions\"][:][\"y\"]\n",
    "    r_z = data[\"ParticleData\"][str(idx)][\"Particles\"][\"Positions\"][:][\"z\"]\n",
    "    r = np.vstack([r_x,r_y,r_z]).transpose()\n",
    "    \n",
    "    # Get IDs\n",
    "    ID = np.array(data[\"ParticleData\"][str(idx)][\"Particles\"][\"IDs\"])\n",
    "    \n",
    "    # Store in single large list\n",
    "    rs.append(r * 1000.) #convert to m \n",
    "    vs.append(v * 1000.) #convert to m \n",
    "    ids.append(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545b66b",
   "metadata": {},
   "source": [
    "### Now some helper functions for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31024da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many iterations happened between IO\n",
    "iteration_stepsize = iterations_idx[1] - iterations_idx[0]\n",
    "\n",
    "\n",
    "def find_closest_it(array,value):\n",
    "    \"\"\" Returns the index of the closest iteration to the passed iteration value. \n",
    "    E.g if saved iteration were [0,1000,2000] and you pass 500 , it will return 0, for 501 returns 1 etc.\"\"\"\n",
    "    idx = np.searchsorted(array, value+ (iteration_stepsize // 2), side=\"left\")\n",
    "    if idx > 0 and (idx == len(array) or math.fabs(value - array[idx-1]) < math.fabs(value - array[idx])):\n",
    "        return idx - 1\n",
    "    else:\n",
    "        return idx - 1\n",
    "    \n",
    "def get_particle_r_v(ID,it):\n",
    "    \"\"\"From the large rs, vs arrays returns the values particle with a certain ID closest to the passed iteration\"\"\"\n",
    "    it = find_closest_it(iterations_idx,it)\n",
    "    idx = np.argmax(ids[it]==ID)\n",
    "    return rs[it][idx],vs[it][idx]\n",
    "\n",
    "# Code below breaks if constant properties change\n",
    "assert len(data[\"ParticleData\"][\"ConstantProperties\"][0]) == 6\n",
    "constant_props = data[\"ParticleData\"][\"ConstantProperties\"][()]\n",
    "def get_particle_size(ID):\n",
    "    for p in constant_props:\n",
    "        if p[0] == ID:\n",
    "            return p[3]\n",
    "\n",
    "all_sizes = []\n",
    "for id in tqdm(ID):\n",
    "    all_sizes.append(get_particle_size(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da666df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for conjunction tracking as used in simulation\n",
    "# Determines what is in the plots\n",
    "thresholds = [1,2,4,8,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56283033",
   "metadata": {},
   "source": [
    "### Extract the conjunctions from the HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32649b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert conjunctions to a pandas dataframe for convenience\n",
    "conj = pd.DataFrame(columns=[\"P1\",\"P2\",\"Iteration\",\"SquaredDistance\"])\n",
    "\n",
    "collision_keys = data[\"CollisionData\"].keys()\n",
    "for idx,it in tqdm(enumerate(collision_keys),total=len(collision_keys)):\n",
    "    iteration = int(it)\n",
    "    collisions = data[\"CollisionData\"][it][\"Collisions\"]\n",
    "    for collision in collisions:\n",
    "        size_1 = get_particle_size(collision[0])\n",
    "        size_2 = get_particle_size(collision[1])\n",
    "        conj = conj.append({\"P1\":collision[0], \n",
    "                     \"P2\": collision[1], \n",
    "                     \"Iteration\": iteration, \n",
    "                     \"SquaredDistance\": collision[2],\n",
    "                     \"Size_P1[m]\" : size_1,\n",
    "                     \"Size_P2[m]\" : size_2,\n",
    "                     \"$\\kappa$\": np.sqrt(collision[2]) / ((size_1+size_2)/1000.0)},\n",
    "                     \n",
    "                    ignore_index=True)\n",
    "        \n",
    "conj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccca74",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "Throw away all but the closest encounter, also compute actual distance (not squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort ascending by conjunction distance, then drop all but the first to get closest encounter\n",
    "conj = conj.sort_values('SquaredDistance', ascending=True)\n",
    "unique_conj = conj.drop_duplicates(subset=[\"P1\",\"P2\"],keep=\"first\")\n",
    "unique_conj[\"Distance\"] = np.sqrt(unique_conj.SquaredDistance) * 1000. # Compute distance in meters\n",
    "del conj # deleting to not accidentally use it\n",
    "unique_conj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae0da2",
   "metadata": {},
   "source": [
    "## 2. Conjunction Plots\n",
    "We start with counting the number of conjunctions for different thresholds over the simulation time and then threshold vs total # of conjunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine time granularity\n",
    "steps = 500\n",
    "t = np.linspace(0,end_t.mjd - starting_t.mjd,steps)\n",
    "it = np.linspace(0,max_iterations,steps)\n",
    "summed_conjs = [[] for _ in thresholds]\n",
    "\n",
    "# Compute number of conjunctions\n",
    "for i in tqdm(it):\n",
    "    for idx,threshold in enumerate(thresholds):\n",
    "        count = len(unique_conj[(unique_conj.Iteration < i) & (unique_conj[\"$\\kappa$\"] < threshold)])\n",
    "        summed_conjs[idx].append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9628720",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,5),dpi=300)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "cmap = sns.color_palette(\"tab10\",len(thresholds))\n",
    "\n",
    "# Iterate over thresholds and plot for each\n",
    "for idx,row in enumerate(thresholds):\n",
    "    sns.lineplot(t,summed_conjs[idx],linewidth=3, color = cmap[idx])\n",
    "    \n",
    "plt.legend([\"$\\kappa=$\"+str(t) for t in thresholds],loc='center', bbox_to_anchor=(1.15,0.5), ncol=1, fancybox=False, shadow=False)\n",
    "# plt.title(\"Conjunction Thresholds Comparison\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"# of Conjunctions\")\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute conjunction counts at specific points in the simulation\n",
    "steps = 500 # threshold granularity\n",
    "max_threshold = 10 # maximum investigated threshold\n",
    "# Time axis\n",
    "threshold_grid = np.logspace(-1,np.log10(max_threshold),steps)\n",
    "sums = []\n",
    "for idx,threshold in enumerate(threshold_grid):\n",
    "    count = len(unique_conj[(unique_conj[\"$\\kappa$\"] < threshold)])\n",
    "    sums.append(count)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5),dpi=300)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "sns.lineplot(threshold_grid,sums,linewidth=3,color=cmap[0])\n",
    "    \n",
    "# plt.title(\"Conjunction Thresholds vs. \\n # of Conjunctions\")\n",
    "plt.xlabel(\"Conjunction Threshold $\\kappa$\")\n",
    "plt.ylabel(\"# of Conjunctions\")\n",
    "#plt.gca().set_xscale(\"log\")\n",
    "#plt.gca().set_yscale(\"log\")\n",
    "# plt.grid()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d57adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = np.concatenate([unique_conj[\"Size_P1[m]\"],unique_conj[\"Size_P2[m]\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f582807",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = (total_days+1) / 365\n",
    "years = list(range(int(years)))\n",
    "years = [str(year+1) for year in years]\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61103243",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_for_year = np.linspace(0,max_iterations,len(years)+1)\n",
    "iterations_for_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bcdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_cutoffs = [0,9460500,18921000]\n",
    "N = [sum(np.logical_and(unique_conj['Iteration'] < cut,unique_conj['Iteration'] > last_cut)) for cut,last_cut in zip(year_cutoffs[1:],year_cutoffs[:-1])]\n",
    "year_labels = [\"0-3 N=\"+str(N[0]),\"3-6 N=\"+str(N[1])]\n",
    "print(year_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be2df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_conj['Year'] = pd.cut(unique_conj['Iteration'],year_cutoffs,labels=year_labels)\n",
    "fig = plt.figure(figsize=(6,6),dpi=300)\n",
    "fig.patch.set_facecolor('white')\n",
    "ax = plt.gca()\n",
    "cmap = sns.color_palette(\"magma\", as_cmap=True)\n",
    "sns.scatterplot(data=unique_conj,x=\"Size_P1[m]\",y=\"Size_P2[m]\",hue=\"$\\kappa$\",style=\"Year\",ax=ax,palette=cmap)\n",
    "sns.scatterplot(data=unique_conj,x=\"Size_P2[m]\",y=\"Size_P1[m]\",hue=\"$\\kappa$\",style=\"Year\",ax=ax,legend=None,palette=cmap)\n",
    "plt.xlabel(\"Size Colliding Particle #1 [m]\")\n",
    "plt.ylabel(\"Size Colliding Particle #2 [m]\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of collisions for each orbital element\n",
    "fig = plt.figure(figsize=(6,6),dpi=300)\n",
    "fig.patch.set_facecolor('white')\n",
    "N_bins = 20\n",
    "\n",
    "cmap = sns.color_palette(\"tab10\",len(thresholds))\n",
    "\n",
    "ax = sns.histplot(data=sizes,bins=N_bins,binrange=[np.log10(0.05),np.log10(max(all_sizes))],\n",
    "                  kde=True,log_scale=(True,False),stat=\"percent\",color=cmap[0])\n",
    "sns.histplot(data=all_sizes,bins=N_bins,binrange=[np.log10(0.05),np.log10(max(all_sizes))],\n",
    "             kde=True,log_scale=(True,False), ax=ax,stat=\"percent\",color=cmap[1])\n",
    "plt.xlim([0.05,max(all_sizes)])\n",
    "plt.xlabel(\"Size [m]\")\n",
    "plt.ylabel(\"Percentage [%]\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend([\"Particles in Conjunctions\", \"All Particles\"])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f9afbd",
   "metadata": {},
   "source": [
    "### Let's investigate the relationship of orbital elements and conjunctions (below some threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd393a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = [[],[],[],[],[],[]]\n",
    "threshold = 100 #cutoff for investigated conjunctions\n",
    "for idx,row in tqdm(unique_conj.iterrows(),total=len(unique_conj.index)):\n",
    "    if row.Distance < threshold:\n",
    "        r,v = get_particle_r_v(row.P1,row.Iteration)\n",
    "        a,e,i,W,w,E = pk.ic2par(r.astype(\"double\"),v.astype(\"double\"), pk.MU_EARTH)\n",
    "        elements[0].append(abs(a))\n",
    "        elements[1].append(abs(e))\n",
    "        elements[2].append(abs(i))\n",
    "        elements[3].append(abs(W))\n",
    "        elements[4].append(abs(w))\n",
    "        elements[5].append(abs(E))\n",
    "        \n",
    "        r,v = get_particle_r_v(row.P2,row.Iteration)\n",
    "        a,e,i,W,w,E = pk.ic2par(r.astype(\"double\"),v.astype(\"double\"), pk.MU_EARTH)\n",
    "        elements[0].append(abs(a))\n",
    "        elements[1].append(abs(e))\n",
    "        elements[2].append(abs(i))\n",
    "        elements[3].append(abs(W))\n",
    "        elements[4].append(abs(w))\n",
    "        elements[5].append(abs(E))\n",
    "print(\"Collected data for \",len(elements[0]),\" conjunctions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ed433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute orbital elements for the entire population at t=0 as a reference\n",
    "pop_elements = [[],[],[],[],[],[]]\n",
    "mid_it = len(rs) // 2\n",
    "for r_i,v_i in tqdm(zip(rs[mid_it],vs[mid_it])):\n",
    "    a,e,i,W,w,E = pk.ic2par(r_i.astype(\"double\"),v_i.astype(\"double\"), pk.MU_EARTH)\n",
    "    pop_elements[0].append(abs(a))\n",
    "    pop_elements[1].append(abs(e))\n",
    "    pop_elements[2].append(abs(i))\n",
    "    pop_elements[3].append(abs(W))\n",
    "    pop_elements[4].append(abs(w))\n",
    "    pop_elements[5].append(abs(E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf5195",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot histogram of collisions for each orbital element\n",
    "for idx,element in enumerate([\"a [m]\",\"e\",\"i [rad]\",\"W\",\"w\",\"E\"]):\n",
    "    fig = plt.figure(figsize=(6,6),dpi=300)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    bins = np.linspace(min(pop_elements[idx]),max(pop_elements[idx]),32)\n",
    "    cmap = sns.color_palette(\"tab10\",len(thresholds))\n",
    "    sns.histplot(pop_elements[idx],bins=bins,kde=True,stat = \"percent\",color=cmap[0])\n",
    "    sns.histplot(elements[idx],bins=bins,kde=True,stat = \"percent\",color=cmap[1])\n",
    "    plt.legend([\"All particles at \"+r\"$\\tau = $\"+str(years[len(years)//2-1])+\" years\",f\"Particles in conjunctions\"])\n",
    "#     plt.title(\"Histogram of \"+element)\n",
    "    plt.xlabel(element)\n",
    "    plt.ylabel(\"Relative Frequency [%]\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7fac8",
   "metadata": {},
   "source": [
    "## 3. Dynamics\n",
    "### Now let's plot orbital elements of all particles over the simulation as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce842a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute min, max and mean of orbital elements over simulation\n",
    "min_elements = [[],[],[],[],[],[]]\n",
    "mean_elements = [[],[],[],[],[],[]]\n",
    "max_elements = [[],[],[],[],[],[]]\n",
    "\n",
    "steps = list(range(len(rs)))\n",
    "stepsize = 10\n",
    "for idx in tqdm(steps[::stepsize]): #pick every n-th step\n",
    "    elements = [[],[],[],[],[],[]]\n",
    "    v_it,r_it = vs[idx],rs[idx]\n",
    "    for v,r in zip(v_it,r_it):\n",
    "        a,e,i,W,w,E = pk.ic2par(r.astype(\"double\"),v.astype(\"double\"), pk.MU_EARTH)\n",
    "        elements[0].append(abs(a))\n",
    "        elements[1].append(abs(e))\n",
    "        elements[2].append(abs(i))\n",
    "        elements[3].append(abs(W))\n",
    "        elements[4].append(abs(w))\n",
    "        elements[5].append(abs(E))\n",
    "    for i in range(6):\n",
    "        min_elements[i].append(np.min(elements[i]))\n",
    "        mean_elements[i].append(np.mean(elements[i]))\n",
    "        max_elements[i].append(np.max(elements[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b8a1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Time axis of the simulation\n",
    "t = np.linspace(0,end_t.mjd - starting_t.mjd,len(mean_elements[0]))\n",
    "\n",
    "# Plot for each orbital element\n",
    "for idx,element in enumerate([\"a [m]\",\"e\",\"i [rad]\",\"W [rad]\",\"w [rad]\",\"E [rad]\"]):\n",
    "    fig = plt.figure(figsize=(6,4),dpi=100)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(t,mean_elements[idx],linewidth=1)\n",
    "#     plt.plot(t,min_elements[idx],linewidth=1)\n",
    "#     plt.plot(t,max_elements[idx],linewidth=1)\n",
    "#     plt.legend([\"mean\",\"min\",\"max\"],loc='upper center', bbox_to_anchor=(1.1,0.8), ncol=1, fancybox=True, shadow=True)\n",
    "    plt.title(\"Evolution of \"+element)\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Mean \" + element)\n",
    "    plt.tight_layout()\n",
    "#         plt.gca().set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d9557",
   "metadata": {},
   "source": [
    "### Finally, we will plot the distribution of the distance to the next particle over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a KNN to get distance to nearest neighbors over simulation\n",
    "\n",
    "all_distances = []\n",
    "\n",
    "r_subset = rs[::10] #pick every n-th step\n",
    "\n",
    "for r_it in tqdm(r_subset,total=len(r_subset)):\n",
    "    elements = [[],[],[],[],[],[]]\n",
    "    knn = NearestNeighbors(n_neighbors=2).fit(r_it)\n",
    "    distances,_ = knn.kneighbors(r_it)\n",
    "    distances = distances[:,1] / 1000 # convert to km\n",
    "    all_distances.append(distances)\n",
    "    \n",
    "all_distances = np.asarray(all_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db736f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample to look only at those below some threshold\n",
    "small_distances = []\n",
    "max_dist = 16\n",
    "for dist in all_distances:\n",
    "    small_distances.append(dist[dist < max_dist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distributions for each iteration\n",
    "bins = 64\n",
    "x = np.linspace(0, max_dist, bins)\n",
    "y = np.linspace(0, total_days * pk.DAY2YEAR, len(all_distances))\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = []\n",
    "for dist in tqdm(small_distances):\n",
    "    hist,bin_vals = np.histogram(dist, bins = x,density=False)\n",
    "    hist = np.cumsum(hist)\n",
    "    hist = np.concatenate([hist,[hist[-1]]]) # last value remains for bucket\n",
    "    Z.append(hist)\n",
    "x = np.concatenate([[0],x])\n",
    "Z = np.asarray(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4a070",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create beautiful plots\n",
    "fig = plt.figure(figsize = (8,8),dpi=100)\n",
    "fig.patch.set_facecolor('white')\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, Z,  rstride=1, cstride=1, cmap=\"plasma\", edgecolor='none')\n",
    "ax.view_init(elev=20., azim=120)\n",
    "ax.set_xlabel('Closest Distance [km]')\n",
    "ax.set_ylabel('Years')\n",
    "ax.set_zlabel('Cumulative Frequency');\n",
    "ax.set_xlim([0,max_dist])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee87f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
